{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60212f41-5b3c-4420-b12e-8b874b43d74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import copy\n",
    "from tqdm.notebook import tqdm\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    f1_score, \n",
    "    classification_report\n",
    ")\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModel,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    AutoConfig\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c75d07dd-4524-4519-869c-d88a1e83e41b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>review_id</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>decision</th>\n",
       "      <th>processed_review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4645</td>\n",
       "      <td>iclr20_443_3</td>\n",
       "      <td>\"In this paper, the authors propose to learn s...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>paper author propose learn surrogate loss func...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1918</td>\n",
       "      <td>iclr19_815_2</td>\n",
       "      <td>\"Summary: by assuming a correct, strongly fact...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>summary assuming correct strongly factored env...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6918</td>\n",
       "      <td>iclr20_1425_2</td>\n",
       "      <td>\"This paper proposes a method to learn graph f...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>paper proposes method learn graph feature mean...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>58</td>\n",
       "      <td>iclr19_25_2</td>\n",
       "      <td>\"This work extends the approximate nearest nei...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>work extends approximate nearest neighbor sear...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3328</td>\n",
       "      <td>iclr19_1410_1</td>\n",
       "      <td>\"Summary ======== The paper focuses on memory ...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>summary paper focus memory management problem ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0      review_id  \\\n",
       "0        4645   iclr20_443_3   \n",
       "1        1918   iclr19_815_2   \n",
       "2        6918  iclr20_1425_2   \n",
       "3          58    iclr19_25_2   \n",
       "4        3328  iclr19_1410_1   \n",
       "\n",
       "                                              review  rating  decision  \\\n",
       "0  \"In this paper, the authors propose to learn s...       2         0   \n",
       "1  \"Summary: by assuming a correct, strongly fact...       3         1   \n",
       "2  \"This paper proposes a method to learn graph f...       2         0   \n",
       "3  \"This work extends the approximate nearest nei...       2         0   \n",
       "4  \"Summary ======== The paper focuses on memory ...       2         0   \n",
       "\n",
       "                                    processed_review sentiment  \n",
       "0  paper author propose learn surrogate loss func...  negative  \n",
       "1  summary assuming correct strongly factored env...  positive  \n",
       "2  paper proposes method learn graph feature mean...  negative  \n",
       "3  work extends approximate nearest neighbor sear...  negative  \n",
       "4  summary paper focus memory management problem ...  negative  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('train.csv')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f85bef3b-120d-4939-abac-19ef375784e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1e67b74-d12f-49d0-bcd0-ede37bb06b13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 500 artists>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAQp0lEQVR4nO3dXawcZ33H8e+vdggvoSJpTqLUtmpTWbQOaguyUtpUCBFozItwLorkSiCrSpWb0EJbCTlCKupFJFpViF40lSygsgQlinhRLFApkQFVvUnqkNDGMW4MSRPXbmyKKLQXgYR/L3ZS1vY5Putzds/OPPv9SEc78+zs2ef/zMxv5sy+nFQVkqS2/My8OyBJmj7DXZIaZLhLUoMMd0lqkOEuSQ3aPO8OAFx77bW1ffv2eXdDkgbl4Ycf/m5VLS13Xy/Cffv27Rw9enTe3ZCkQUny7yvd52UZSWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lq0EKG+/YDX5p3FyRpphYy3CWpdYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkho0Ubgn+aMkx5I8luQzSV6a5JokDyR5oru9emz5u5KcTHIiya2z674kaTmrhnuSLcAfArur6rXAJmAfcAA4UlU7gSPdPEl2dfffCOwB7kmyaTbdlyQtZ9LLMpuBlyXZDLwcOA3sBQ519x8Cbuum9wL3VtVzVfUkcBK4aWo9liStatVwr6r/AP4SeBo4A/x3VX0FuL6qznTLnAGu6x6yBXhm7Fec6trOk+SOJEeTHD137tz6qpAknWeSyzJXMzob3wH8PPCKJO+51EOWaauLGqoOVtXuqtq9tLQ0aX8ljfG/imklk1yWeQvwZFWdq6ofA58HfhN4NskNAN3t2W75U8C2scdvZXQZR5K0QSYJ96eBNyR5eZIAtwDHgcPA/m6Z/cD93fRhYF+SK5PsAHYCD02325KkS9m82gJV9WCSzwLfAJ4HHgEOAlcB9yW5ndEB4N3d8seS3Ac83i1/Z1W9MKP+S5KWsWq4A1TVh4EPX9D8HKOz+OWWvxu4e31dkyStlZ9QlaQGGe6S1CDDXZIaZLhLPeb72LVWhrskNchwl6QGGe6S1CDDXRoYr8NrEoa7JDXIcJekBhnuktQgw12SGmS4S1oTX9jtN8NdkhpkuEtSgwz3jn9iSmqJ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwv4H9kktQCw12SGmS4S1KDDHf1jpfGpPUz3CWpQYa7JDVoonBP8qokn03yrSTHk/xGkmuSPJDkie726rHl70pyMsmJJLfOrvuSpOVMeub+V8CXq+qXgF8FjgMHgCNVtRM40s2TZBewD7gR2APck2TTtDsuSVrZquGe5GeBNwKfAKiqH1XV94G9wKFusUPAbd30XuDeqnquqp4ETgI3TbfbkqRLmeTM/dXAOeBvkzyS5ONJXgFcX1VnALrb67rltwDPjD3+VNd2niR3JDma5Oi5c+fWVYQk6XyThPtm4PXA31TV64D/pbsEs4Is01YXNVQdrKrdVbV7aWlpos5KkiYzSbifAk5V1YPd/GcZhf2zSW4A6G7Pji2/bezxW4HT0+luf/nebEl9smq4V9V/As8keU3XdAvwOHAY2N+17Qfu76YPA/uSXJlkB7ATeGiqvZakgZjXid/mCZf7A+DTSV4CfAf4PUYHhvuS3A48DbwboKqOJbmP0QHgeeDOqnph6j2XJK1oonCvqkeB3cvcdcsKy98N3L32bkmS1sNPqEpSgwx3SWqQ4S5JDTLcJalBhrs2jJ8FkDaO4S5JDTLcJalBhrskNchwl6QGGe6r8EVASUNkuEtSgwx3SWqQ4S5JDTLcJalBhrukQfFNDpMx3CWpQYa7JDXIcJekBhnuks7jNe02GO6S1CDDXZIaZLhLUoMMd0lqkOG+AXyBStJGM9wlqUGGu7QM/9rS0BnuktQgw12SGmS4a+68BCJNn+Euaa48uM+G4S5JDTLc1Suexa2dY6dxhrskTVFfDrKGuyQ1yHAfmL6cFUjqN8NdgAeNaXAM1SeG+xS5c0vqC8Ndkho0cbgn2ZTkkSRf7OavSfJAkie626vHlr0ryckkJ5LcOouOS5JWdjln7u8Hjo/NHwCOVNVO4Eg3T5JdwD7gRmAPcE+STdPpbru8pDM8rrOfciz6Z6JwT7IVeAfw8bHmvcChbvoQcNtY+71V9VxVPQmcBG6aSm8lSROZ9Mz9Y8AHgZ+MtV1fVWcAutvruvYtwDNjy53q2s6T5I4kR5McPXfu3OX2W5J0CauGe5J3Amer6uEJf2eWaauLGqoOVtXuqtq9tLQ04a/WvPnntzQMk5y53wy8K8lTwL3Am5N8Cng2yQ0A3e3ZbvlTwLaxx28FTk+tx5IGzROEjbFquFfVXVW1taq2M3qh9KtV9R7gMLC/W2w/cH83fRjYl+TKJDuAncBDU++5JGlFm9fx2I8A9yW5HXgaeDdAVR1Lch/wOPA8cGdVvbDunkqSJnZZH2Kqqq9X1Tu76f+qqluqamd3+72x5e6uql+sqtdU1d9Pu9PSUHlJQhvFT6hKmgoPXP1iuPfIRu0c7oRS+wx3aUIeFNuxCOvScJd6ZBFCRxvDcJ8yd05dLreZlTk2a2e4S1KDDPcF5lmR1C7DXbqEWR0APbAOyxDXl+EudYa4A0/TItbfcs2GuyQ1yHCXpAYZ7mvU8p9zkobPcNfgeGCVVme4a/AMe+lihvucGEiSZslwHzAPEG1yvWoaDHdJapDhPiOefUkby33ufIa7muXOrkVmuGtVhuTsOcaaNsO9Z1rZyVupQxoqw10XWbRgXrR6h8x1NTnDXU1x55dGDHdJmsDQThwM90YMbcOTNFuGu9RDHqzbtJHrdeHC3Z1mOhZpHBep1llw/OZj4cJ9iBZl51iUOqWNYLhrRYsatq3WPYu65jVWra6jaTLc1Qut7ayt1aP1mcf2sDDh3sedrY99ktSGhQl3jXhAUV+4Lc5WE+HuRiJdWov7SF9q6ks/LtREuEsaeTFo+ho42jiGe8PcwTUrblsX69uYGO49sN6Nom8bVV/Napwm/b3zWE++VXFxGe6SVmRID1fT4e6GKWmthp4fq4Z7km1JvpbkeJJjSd7ftV+T5IEkT3S3V4895q4kJ5OcSHLrLAvQ9Ax9Y57UotSpxTbJmfvzwJ9U1S8DbwDuTLILOAAcqaqdwJFunu6+fcCNwB7gniSbZtH5jXSpQBhiWAyxz7o8Q1jHQ+jjUK0a7lV1pqq+0U3/EDgObAH2Aoe6xQ4Bt3XTe4F7q+q5qnoSOAncNOV+S1KvzfvAdVnX3JNsB14HPAhcX1VnYHQAAK7rFtsCPDP2sFNd24W/644kR5McPXfu3Bq63l/TWKnz3jAu1Lf+LJrtB7600OtgkWtfq4nDPclVwOeAD1TVDy616DJtdVFD1cGq2l1Vu5eWlibthtZonjuHO+ZstDSuLdXSFxOFe5IrGAX7p6vq813zs0lu6O6/ATjbtZ8Cto09fCtwejrdlfrFUFJfTfJumQCfAI5X1UfH7joM7O+m9wP3j7XvS3Jlkh3ATuCh6XVZUl+1cLDbyBpm+VyTnLnfDLwXeHOSR7uftwMfAd6a5Angrd08VXUMuA94HPgycGdVvTCT3qvXWtjRNUxue7B5tQWq6p9Y/jo6wC0rPOZu4O519EsTciNeu+0HvsRTH3nHvLsxdW4TgsY/oXq5Vtsp3Gn6Z1rf6zL0ddvHF8z7MKaL/C4jw32D9XEn7Nvv1Py5XofPcG+cgS7106z3o4UI9xbCqA81TLsPfahJatVChPu0jYeSATUbjuvGcrzb02y4u7FunHn/E4y1PG4I28cQ+qj+ajbc1Q/zDqh5P78WS5+2N8N9GX1aQWsx9P733aKO70bXvajjPC2Gu9RzfQq5Fv7fbx/6sBGaCfe+rrC+9mtSQ+//tL04Hpc7Lo5jf63lDRJDWJ/NhLuk4RtCaL6o73013FfQ9xU3VI7r/LW4Dlqsab0M9wFyQ+4v183srOeSySKuF8NdUzevHWktz7uIO/0i68P63qg+GO5z1oeNbTmz/ABRH36n1Lomw/1ywsDgGAbXk2atz19dvBZNhrvmY6g7waLzu5LaZLg3xp1zOHyNQLPUXLhfuPG7M0haRM2Fu9R3nnBMj2O5MsNdWmCG43T0cRwN90vo4wprxTTGtu/rp+/90+r69Jbgy2W4S1KDDPeBmtaZQR/OMFrnGGseFjrc+77T9b1/0qTcljfeQof7RnMDl7RRDHdJG67vJzp9798kDHdpAbUQXpdr0Wo23NWERdtxpdUY7pJ6xQP1dBjukjShIR14DHcNypB2rlY45sNkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1KCZhXuSPUlOJDmZ5MCsnkeSdLGZhHuSTcBfA28DdgG/m2TXLJ5LknSxWZ253wScrKrvVNWPgHuBvTN6LknSBVJV0/+lye8Ae6rq97v59wK/XlXvG1vmDuCObvY1wIl1POW1wHfX8fghsub2LVq9YM2X6xeqamm5OzavvT+XlGXazjuKVNVB4OBUniw5WlW7p/G7hsKa27do9YI1T9OsLsucAraNzW8FTs/ouSRJF5hVuP8zsDPJjiQvAfYBh2f0XJKkC8zkskxVPZ/kfcA/AJuAT1bVsVk8V2cql3cGxprbt2j1gjVPzUxeUJUkzZefUJWkBhnuktSgQYd7q19xkOSTSc4meWys7ZokDyR5oru9euy+u7oxOJHk1vn0en2SbEvytSTHkxxL8v6uvdm6k7w0yUNJvtnV/Gdde7M1w+gT7EkeSfLFbr7pegGSPJXkX5M8muRo1zbbuqtqkD+MXqj9NvBq4CXAN4Fd8+7XlGp7I/B64LGxtr8ADnTTB4A/76Z3dbVfCezoxmTTvGtYQ803AK/vpl8J/FtXW7N1M/o8yFXd9BXAg8AbWq65q+OPgb8DvtjNN11vV8tTwLUXtM207iGfuTf7FQdV9Y/A9y5o3gsc6qYPAbeNtd9bVc9V1ZPASUZjMyhVdaaqvtFN/xA4Dmyh4bpr5H+62Su6n6LhmpNsBd4BfHysudl6VzHTuocc7luAZ8bmT3Vtrbq+qs7AKAiB67r25sYhyXbgdYzOZJuuu7tE8ShwFnigqlqv+WPAB4GfjLW1XO+LCvhKkoe7r16BGdc9q68f2AirfsXBgmhqHJJcBXwO+EBV/SBZrrzRosu0Da7uqnoB+LUkrwK+kOS1l1h80DUneSdwtqoeTvKmSR6yTNtg6r3AzVV1Osl1wANJvnWJZadS95DP3BftKw6eTXIDQHd7tmtvZhySXMEo2D9dVZ/vmpuvG6Cqvg98HdhDuzXfDLwryVOMLqO+OcmnaLfe/1dVp7vbs8AXGF1mmWndQw73RfuKg8PA/m56P3D/WPu+JFcm2QHsBB6aQ//WJaNT9E8Ax6vqo2N3NVt3kqXujJ0kLwPeAnyLRmuuqruqamtVbWe0v361qt5Do/W+KMkrkrzyxWngt4HHmHXd834VeZ2vQL+d0bsqvg18aN79mWJdnwHOAD9mdBS/Hfg54AjwRHd7zdjyH+rG4ATwtnn3f401/xajPz3/BXi0+3l7y3UDvwI80tX8GPCnXXuzNY/V8SZ++m6Zputl9I6+b3Y/x17MqlnX7dcPSFKDhnxZRpK0AsNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNej/AAzqKWObhwn3AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y = [len(t.split()) for t in train_df['processed_review']]\n",
    "\n",
    "x = range(0, len(y))\n",
    "plt.bar(x, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0de3130b-3bf8-4537-9eb6-0d7df6400d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the values in the 'sentiment' column to 1 for 'positive' and 0 for 'negative'\n",
    "train_df['sentiment'] = train_df['sentiment'].map({'positive': 1, 'negative': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f468a385-b7a1-4589-bf43-838c025501df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['sentiment'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a23038bb-d7ef-4b6c-8efd-701ff6eda5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        super(Config, self).__init__()\n",
    "\n",
    "        self.SEED = 42\n",
    "        self.MODEL_PATH = 'allenai/scibert_scivocab_uncased'\n",
    "        self.NUM_LABELS = 2\n",
    "\n",
    "        # data\n",
    "        self.TOKENIZER = AutoTokenizer.from_pretrained(self.MODEL_PATH)\n",
    "        self.MAX_LENGTH = 320\n",
    "        self.BATCH_SIZE = 16\n",
    "        self.VALIDATION_SPLIT = 0.25\n",
    "\n",
    "        # model\n",
    "        self.DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.FULL_FINETUNING = True\n",
    "        self.LR = 3e-5\n",
    "        self.OPTIMIZER = 'AdamW'\n",
    "        self.CRITERION = 'BCEWithLogitsLoss'\n",
    "        self.N_VALIDATE_DUR_TRAIN = 3\n",
    "        self.N_WARMUP = 0\n",
    "        self.SAVE_BEST_ONLY = True\n",
    "        self.EPOCHS = 1\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad1e9428-34fa-487e-a55f-5a2537e18a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDataset(Dataset):\n",
    "    def __init__(self, df, indices, set_type=None):\n",
    "        super(TransformerDataset, self).__init__()\n",
    "\n",
    "        df = df.iloc[indices]\n",
    "        self.texts = get_texts(df)\n",
    "        self.set_type = set_type\n",
    "        if self.set_type != 'test':\n",
    "            self.labels = get_labels(df)\n",
    "\n",
    "        self.tokenizer = config.TOKENIZER\n",
    "        self.max_length = config.MAX_LENGTH\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        tokenized = self.tokenizer.encode_plus(\n",
    "            self.texts[index], \n",
    "            max_length=self.max_length,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_token_type_ids=False,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = tokenized['input_ids'].squeeze()\n",
    "        attention_mask = tokenized['attention_mask'].squeeze()\n",
    "\n",
    "        if self.set_type != 'test':\n",
    "            label = torch.Tensor([self.labels[index]]).long()  # Convert label to tensor\n",
    "            return {\n",
    "                'input_ids': input_ids,\n",
    "                'attention_mask': attention_mask,\n",
    "                'labels': label,\n",
    "            }\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da38ab27-61a4-4237-95e2-558fa56aef63",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(config.SEED)\n",
    "\n",
    "dataset_size = len(train_df)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(config.VALIDATION_SPLIT * dataset_size))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "train_indices, val_indices = indices[split:], indices[:split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "364a9a17-6ac5-4678-ada0-6d6e726457c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT -\tpaper author propose learn surrogate loss function nondifferentiable nondecomposable loss alternative minimization method used training surrogate network prediction model learning surrogate loss function different task somewhat novel although prior work learning loss eg 1 pro 1 paper well written easy follow 2 learning unified loss different task interesting potential reduce human effort design loss con 1 important detail learning surrogate loss missing function g extracting latent error component clear composition function h provided either 2 detail experiment clear result test set many independent run performed 3 learning surrogate loss incurs additional approximation error time complexity model complexity benefit tradeoff systematically evaluated question 1 function g extracting latent error component clear required design different task expert specifically differentiable form different task please provide detail function g different loss experiment 2 proposed loss work well multiclass classification task experiment binary classification evaluated multiclass classification increase number class thus increasing difficulty approximation better provide mcr compared ce cifar100 dataset evaluation also please provide running time ce slr running environment 3 result table 3 test set ce fewer parameter compared proposed loss slr better generalization performance compared ce many independent run performed experiment 4 ce need training loss compared slr please provide running time ce environment fair comparison 5 time complexity analysis treat extracting function g blackbox function however complexity function g depends task please provide detailed discussion time complexity different task eg auc f1 mcr multiclass classification ranking task 1 learning loss function semisupervised learning via discriminative adversarial network 2017\n",
      "LABEL -\t0\n",
      "\n",
      "TEXT -\tsummary assuming correct strongly factored environment model improved estimator useful policy search derived counterfactual reasoning data sampled experience used refine initial condition model translates improved estimator policy value improves policy search major comment enjoyed paper think modelbased rl deserves work think simple reasonably workable approach nice theoretical benefit like idea scms like idea counterfactual reasoning like idea leveraging model unique way negative side felt paper make rather strong assumption specifically agent access perfect model mismatch model decomposes neatly noise variable plus deterministic function given model one wonder technique say classical planning could also used sort policy search question approximation first see probabilistic inference core element algorithm puh must computed large complex model assume must approximate inference lead naturally question accuracy approximate inference result biased estimator probably yes efficacy inaccuracy inherent approximate inference outweigh benefit using puh v pu scalability large model reasonably cope degradation unacceptable better noncf algorithm far tell none addressed paper although expect every paper answer every question first step wish experiment little varied experimental result really show marginal improvement one small task understand empirical paper neither fit strongly category theory paper example theory result indicating sort benefit might expect using method outlined absence theory might reasonably look various experiment demonstrate effectiveness pro integration scms interesting counterfactual variant algorithm clearly motivated interesting paper generally wellwritten con assumption agent given model mismatch strong model class noise variable deterministic function seems potentially restrictive question impact approximate inference experiment could varied\n",
      "LABEL -\t1\n",
      "\n",
      "TEXT -\tpaper proposes method learn graph feature mean neural network graph classification proposed method graph described bag subgraphs subgraph dictionary learned isomorphic matching author present two approach toward isomorphic matching one brutetoforce approach check node permutation based spectral decomposition toward efficient computation experiment graph classification task using several benchmark datasets learned feature proposed method exhibit favorable performance comparison graphbased method paper leaning toward rejection 1 proposed method lack novelty 2 contains technically imprecise part 3 effectiveness fully validated experiment detailed comment follows presented method belongs standard feature representation framework describes graph bag subgraph template dictionary paper contribution found way learn subgraph dictionary learning convolution kernel cnns contrast cnn graph representation pose challenging issue isomorphism however simply employ brutetoforce approach toward graph isomorphism lacking novelty hand alternative approach relaxes graph matching eq8 spectral decomposition seriously degrades characteristic permutation matrix p thus resulting score z exhibit graph matching measure anymore eq8 lack theoretical justification far away subgraph based representation cannot understand kind feature actually extracted eq8 though author insist method retains explicit graph structural information constraint imposed kernel k embedding graph structure k namely kernel k required exhibit nature adjacency matrix subgraph lack description andor discussion aspect nodeorder information still exists classification layer sec42 since fc classifier directly applied flattened feature map tensor q two ax defined according node order graph contradicts author claim method invariant node ordering accomplishing nodeorderless classification global pooling gap applied final feature map classifier layer addition cannot fully understand stack subgraph based feature extraction sec41 deep manner extracting subgraph representation first resulting matrix feature map c channel adjacency matrix contains pairwise relationship node unclear construct deeper model repeatedly applying subgraph template matching method built upon local kernel k adjacency matrix although invariant node order locally within local kernel method cannot capture subgraph structure beyond locally ordered node locally ordered node exhibit certain subgraph easily spread globally via applying node permutation thus method applicable limited case node order input graph roughly canonicalized paper completely lack discussion analysis limitationassumption method regarding locality experiment classifier module different across comparison method proposed method feature extraction graph fairly compared type graph feature extraction method consistent pipeline basis identical classifier module wl method performance 524 mutag table 1 significantly inferior 8088 reported b wale n watson ia karypis g comparison descriptor space chemical compound retrieval classification knowl inf syst 2008 143 pp347375 b schlichtkrull kipf tn bloem p van den berg r titov welling 2018 modeling relational data graph convolutional network gangemi et al ed semantic web eswc 2018 lecture note computer science vol 10843 springer cham minor comment improper citation format use citep citet properly according context related kernel method graphkernel stringkernel would better mention related kernel function clarifying contribution\n",
      "LABEL -\t0\n",
      "\n",
      "TEXT -\twork extends approximate nearest neighbor search anns algorithm general setting instead search separable similarity measure author propose optimal binary functional search obfs scoring function f general nonseparable exact construction binary function graph wrt f x computationally expensive specific approximate algorithm obfs proposed paper 1 first construct l2 delaunay graph based dataset x 2 perform greedy search l2 delaunay graph author also discus various condition approximation method achieve close optimal value concern work 1 author demonstrate sufficient value performing approximation specific fashion instance theorem 2 author start concavity assumption scoring function f natural apply gradient ascent method neighborhood graph author quantitatively qualitatively justify specific approach 2 lately numerous publication shown distilled model achieve high quality render scoring function separable author least compare method distillation maximum inner product search based approach overall research direction interesting specific work fall short publication iclr\n",
      "LABEL -\t0\n",
      "\n",
      "TEXT -\tsummary paper focus memory management problem memoryaugmented neural network length streaming data much larger number memory entry paper proposes longterm episodic memory network lemn learn rnnbased agent erase le important memory entry storing incoming data computing retention score memory entry based importance relative memory entry rnn memory entry entry historical importance rnn entry hidden value time comment target problem memory management mann importance solution interesting especially spatiotemporal lemn spatial dependency memory slot temporal evolution slot modeled however experiment give proof concept without comparison stateoftheart task example paper lack comparison differentiable neural computer dnc 1 wellknown memoryaugmented neural network since dnc also ability keep track usage information memory entry decide whether free comparison proposed lemn dnc model considered extension dntm 2 referred imlemn paper introduction recurrent connection space time although comparison lemn imlemn available section 42 43 similar comparison section 41 see whether addition recurrent connection brings benefit abbreviation made clear eg mqn written full form using mqn cited oh et al 2016 reference 1 alex graf greg wayne malcolm reynolds tim harley ivo danihelka agnieszka grabskabarwinska sergio gomez colmenarejo edward grefenstette tiago ramalho john agapiou adria puigdomenech badia karl moritz hermann yori zwols georg ostrovski adam cain helen king christopher summerfield phil blunsom koray kavukcuoglu demis hassabis hybrid computing using neural network dynamic external memory nature 5387626 471476 2016 doi 101038nature20101 2 caglar gulcehre sarath chandar kyunghyun cho yoshua bengio dynamic neural turing machine soft hard addressing scheme corr abs160700036 2016\n",
      "LABEL -\t0\n",
      "\n",
      "TEXT -\tpaper introduces genetic algorithm maintains archive representation iteratively evolved selected comparing validation error representation constructed syntax tree consists element common neural network architecture experimental result showed algorithm competitive stateoftheart achieving much smaller model size comment 1 think paper lack technical novelty im going focus experimental result following two question 2 feat typical genetic algorithm converges slowly appendix one verify feat converges least 10x slower xgboost feat achieve lower error xgboost use amount time author provide convergence plot algorithm ie real time v test error 3 figure 3 seems proposed algorithm competitive xgboost model size much smaller xgboost author tried postprocessing model generated xgboost hows performance compare\n",
      "LABEL -\t1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_texts(df):\n",
    "    #texts = df['processed_review'].apply(clean_abstract)\n",
    "    texts = df['processed_review']\n",
    "    texts = texts.values.tolist()\n",
    "    return texts\n",
    "    \n",
    "\n",
    "def get_labels(df):\n",
    "    labels = df.iloc[:, 6:].values\n",
    "    labels = labels.squeeze().tolist()\n",
    "    return labels\n",
    "\n",
    "texts = get_texts(train_df)\n",
    "labels = get_labels(train_df)\n",
    "\n",
    "for text, label in zip(texts[:6],labels[:6]):\n",
    "    print(f'TEXT -\\t{text}')\n",
    "    print(f'LABEL -\\t{label}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8daf390-eabc-48de-a56b-a2b887fc9109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids shape: torch.Size([16, 320])\n",
      "attention_mask shape: torch.Size([16, 320])\n",
      "labels shape: torch.Size([16, 1])\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = [item['input_ids'] for item in batch]\n",
    "    attention_masks = [item['attention_mask'] for item in batch]\n",
    "    labels = [item['labels'] for item in batch]\n",
    "\n",
    "    # Pad sequences to the maximum length in the batch\n",
    "    input_ids = pad_sequence(input_ids, batch_first=True)\n",
    "    attention_masks = pad_sequence(attention_masks, batch_first=True)\n",
    "    labels = torch.stack(labels, dim=0)\n",
    "\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_masks,\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "train_data = TransformerDataset(train_df, train_indices)\n",
    "val_data = TransformerDataset(train_df, val_indices)\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=config.BATCH_SIZE, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_data, batch_size=config.BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "b = next(iter(train_dataloader))\n",
    "for k, v in b.items():\n",
    "    print(f'{k} shape: {v.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f4b2f8-b6e2-4f7d-8597-464e223e24cb",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8b413f10-f1a7-41aa-9207-66ad01c5807d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.transformer_model = AutoModel.from_pretrained(config.MODEL_PATH)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.output = nn.Linear(768, 2)  # Adjust the output size for binary classification\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n",
    "        _, pooled_output = self.transformer_model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.output(pooled_output)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a4945b1-3612-4b9c-a453-44f795d233b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = config.DEVICE\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47066dfa-0859-4eac-82e9-61abd8df5898",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(model, val_dataloader, criterion):\n",
    "    \n",
    "    val_loss = 0\n",
    "    true, pred = [], []\n",
    "    \n",
    "    # set model.eval() every time during evaluation\n",
    "    model.eval()\n",
    "    \n",
    "    for step, batch in enumerate(val_dataloader):\n",
    "        # unpack the batch contents and push them to the device (cuda or cpu).\n",
    "        b_input_ids = batch['input_ids'].to(device)\n",
    "        b_attention_mask = batch['attention_mask'].to(device)\n",
    "        b_labels = batch['labels'].to(device)\n",
    "\n",
    "        # using torch.no_grad() during validation/inference is faster -\n",
    "        # - since it does not update gradients.\n",
    "        with torch.no_grad():\n",
    "            # forward pass\n",
    "            logits = model(input_ids=b_input_ids, attention_mask=b_attention_mask)\n",
    "\n",
    "            # apply sigmoid activation function to the logits\n",
    "            logits = torch.sigmoid(logits)\n",
    "\n",
    "            # round the logits to get the predicted labels\n",
    "            logits_rounded = torch.round(logits)\n",
    "\n",
    "            # convert the tensors to numpy arrays\n",
    "            logits_numpy = logits_rounded.cpu().numpy()\n",
    "            labels_numpy = b_labels.cpu().numpy()\n",
    "\n",
    "            # append the predicted labels and true labels to the respective lists\n",
    "            pred.extend(logits_numpy)\n",
    "            true.extend(labels_numpy)\n",
    "\n",
    "            # calculate loss\n",
    "            loss = criterion(logits, b_labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_dataloader)\n",
    "    print('Val loss:', avg_val_loss)\n",
    "    print('Val accuracy:', accuracy_score(true, pred))\n",
    "\n",
    "    val_micro_f1_score = f1_score(true, pred, average='micro')\n",
    "    print('Val micro f1 score:', val_micro_f1_score)\n",
    "    return val_micro_f1_score\n",
    "\n",
    "\n",
    "def train(model, train_dataloader, val_dataloader, criterion, optimizer, scheduler, epoch):\n",
    "    \n",
    "    # we validate config.N_VALIDATE_DUR_TRAIN times during the training loop\n",
    "    nv = config.N_VALIDATE_DUR_TRAIN\n",
    "    temp = len(train_dataloader) // nv\n",
    "    temp = temp - (temp % 100)\n",
    "    validate_at_steps = [temp * x for x in range(1, nv + 1)]\n",
    "    \n",
    "    train_loss = 0\n",
    "    for step, batch in enumerate(tqdm(train_dataloader, \n",
    "                                      desc='Epoch ' + str(epoch))):\n",
    "        # set model.eval() every time during training\n",
    "        model.train()\n",
    "        \n",
    "        # unpack the batch contents and push them to the device (cuda or cpu).\n",
    "        b_input_ids = batch['input_ids'].to(device)\n",
    "        b_attention_mask = batch['attention_mask'].to(device)\n",
    "                \n",
    "        b_labels = batch['labels'].to(device)\n",
    "                        \n",
    "        # clear accumulated gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward pass\n",
    "        logits = model(input_ids=b_input_ids, attention_mask=b_attention_mask)\n",
    "        \n",
    "        # Adjust the shape of the target tensor\n",
    "        b_labels = b_labels.unsqueeze(1).repeat(1, b_input_ids.size(1), 2)\n",
    "                      \n",
    "        # calculate loss\n",
    "        loss = criterion(logits, b_labels)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        # update scheduler\n",
    "        scheduler.step()\n",
    "\n",
    "        if step in validate_at_steps:\n",
    "            print(f'-- Step: {step}')\n",
    "            _ = val(model, val_dataloader, criterion)\n",
    "    \n",
    "    avg_train_loss = train_loss / len(train_dataloader)\n",
    "    print('Training loss:', avg_train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b3049ff-8b5a-4c6b-9886-2ff04581a495",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    # setting a seed ensures reproducible results.\n",
    "    # seed may affect the performance too.\n",
    "    torch.manual_seed(config.SEED)\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    # define the parameters to be optmized -\n",
    "    # - and add regularization\n",
    "    if config.FULL_FINETUNING:\n",
    "        param_optimizer = list(model.named_parameters())\n",
    "        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_parameters = [\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n",
    "                ],\n",
    "                \"weight_decay\": 0.001,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n",
    "                ],\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "        optimizer = optim.AdamW(optimizer_parameters, lr=config.LR)\n",
    "\n",
    "    num_training_steps = len(train_dataloader) * config.EPOCHS\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=num_training_steps\n",
    "    )\n",
    "\n",
    "    max_val_micro_f1_score = float('-inf')\n",
    "    for epoch in range(config.EPOCHS):\n",
    "        train(model, train_dataloader, val_dataloader, criterion, optimizer, scheduler, epoch)\n",
    "        val_micro_f1_score = val(model, val_dataloader, criterion)\n",
    "\n",
    "        if config.SAVE_BEST_ONLY:\n",
    "            if val_micro_f1_score > max_val_micro_f1_score:\n",
    "                best_model = copy.deepcopy(model)\n",
    "                best_val_micro_f1_score = val_micro_f1_score\n",
    "\n",
    "                model_name = 'scibertfft_best_model'\n",
    "                torch.save(best_model.state_dict(), model_name + '.pt')\n",
    "\n",
    "                print(f'--- Best Model. Val loss: {max_val_micro_f1_score} -> {val_micro_f1_score}')\n",
    "                max_val_micro_f1_score = val_micro_f1_score\n",
    "\n",
    "    return best_model, best_val_micro_f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "36cd3825-fed8-4d89-b783-369335956027",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (transformer_model): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(31090, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (output): Linear(in_features=245760, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "38fd7bf9-9d41-4f01-b8ac-97bd934dec91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "885c976d8ec84b62811666adbffe2845",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 0:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (16x768 and 245760x2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_37664/1899696478.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbest_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_val_micro_f1_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_37664/3714229858.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m()\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[0mmax_val_micro_f1_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'-inf'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m         \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m         \u001b[0mval_micro_f1_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_37664/437616405.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, train_dataloader, val_dataloader, criterion, optimizer, scheduler, epoch)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[1;31m# forward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mb_input_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mb_attention_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[1;31m# Adjust the shape of the target tensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1499\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1502\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_37664/815872880.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mpooled_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpooled_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpooled_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlogits\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1499\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1502\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (16x768 and 245760x2)"
     ]
    }
   ],
   "source": [
    "best_model, best_val_micro_f1_score = run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
